{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02d0597e-db99-43a9-9a3c-588907ab7c55",
   "metadata": {},
   "source": [
    "# Deep ANN: Manual Feature Engineering\n",
    "\n",
    "In this notebook, we  will build a data pipeline using using [Tensorflow Extended (TFX)](https://www.tensorflow.org/tfx) to prepare features from the dataset:\n",
    "\n",
    "* created an InteractiveContext to run TFX components interactively\n",
    "* used TFX ExampleGen component to split your dataset into training and evaluation datasets\n",
    "* generated the statistics and the schema of your dataset using TFX StatisticsGen and SchemaGen components\n",
    "* validated the evaluation dataset statistics using TFX ExampleValidator\n",
    "* performed feature engineering using the TFX Transform component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952280d9-901b-4529-8adb-5482d5522d5a",
   "metadata": {},
   "source": [
    "## Package Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117b8344-59be-4059-8492-1f6ae280a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "import tempfile\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "# ignore tf warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e133f7e-487d-45a5-ac7c-fa35a8fa1b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570c122-b235-4328-90d1-5f33c89678e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and evaluation datasets\n",
    "df = pd.read_csv('data/A_E_Fire_Dataset.csv', skipinitialspace=True)\n",
    "\n",
    "# Split the dataset.\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12ab19-0c83-4d2a-b961-f364498bdf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the train set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365d246-b9c8-417c-b7f1-eddfbe17f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the eval set\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17dbfbe-317f-4dbc-b39c-1d06a4e3132f",
   "metadata": {},
   "source": [
    "From these few columns, you can get a first impression of the data. You will notice that most are strings and integers. The STATUS columns contains only zeros and ones. In the next sections, you will see how to use TFDV to aggregate and process this information so you can inspect it more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91eee0-4c83-4314-af5c-0265c1dd57cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generate and visualize training dataset statistics\n",
    "Now we feed the Pandas Dataframe and compute the dataset statistics by using the [`generate_statistics_from_dataframe()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe) method. Under the hood, it distributes the analysis via [Apache Beam](https://beam.apache.org/) which allows it to scale over large datasets.\n",
    "\n",
    "The results returned by this step for numerical and categorical data are summarized in this table:\n",
    "\n",
    "| Numerical Data | Categorical Data   |\n",
    "|:-:|:-:|\n",
    "|Count of data records|Count of data records\n",
    "|% of missing data records|% of missing data records|\n",
    "|Mean, std, min, max|unique records|\n",
    "|% of zero values|Avg string length|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4224b-870d-435c-b384-a74f0c63af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset statistics\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945527f0-5532-4dbc-a861-888b45e54bc7",
   "metadata": {},
   "source": [
    "Now to check for missing data or high standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c66aab-7d7a-4a9c-867d-338114c46de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training dataset statistics\n",
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ec629-56fd-48b8-8ccc-e93123f3829c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Infer data schema\n",
    "\n",
    "\n",
    "Next step is to create a data schema to describe your train set. Simply put, a schema describes standard characteristics of your data such as column data types and expected data value range. The schema is created on a dataset that you consider as reference, and can be reused to validate other incoming datasets.\n",
    "\n",
    "With the computed statistics, TFDV allows you to automatically generate an initial version of the schema using the [`infer_schema()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) method. This returns a Schema [protocol buffer](https://developers.google.com/protocol-buffers) containing the result. As mentioned in the [TFX paper](http://stevenwhang.com/tfx_paper.pdf) (Section 3.3), the results of the schema inference can be summarized as follows:\n",
    "\n",
    "* The expected type of each feature.\n",
    "* The expected presence of each feature, in terms of a minimum count and fraction of examples that must contain\n",
    "the feature.\n",
    "* The expected valency of the feature in each example, i.e.,\n",
    "minimum and maximum number of values.\n",
    "* The expected domain of a feature, i.e., the small universe of\n",
    "values for a string feature, or range for an integer feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e911d-643f-455c-aa55-447834d35d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer schema from the computed statistics.\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "\n",
    "# Display the inferred schema\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d548e5-37a8-410a-a230-a8b78b9f814d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generate and visualize evaluation dataset statistics\n",
    "\n",
    "The next step after generating the schema is to now look at the evaluation dataset. You will begin by computing its statistics then compare it with the training statistics. It is important that the numerical and categorical features of the evaluation data belongs roughly to the same range as the training data. Otherwise, you might have distribution skew that will negatively affect the accuracy of your model.\n",
    "\n",
    "TFDV allows you to generate both the training and evaluation dataset statistics side-by-side. You can use the [`visualize_statistics()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) function and pass additional parameters to overlay the statistics from both datasets (referenced as left-hand side and right-hand side statistics). Let's see what these parameters are:\n",
    "\n",
    "- `lhs_statistics`: Required parameter. Expects an instance of `DatasetFeatureStatisticsList `.\n",
    "\n",
    "\n",
    "- `rhs_statistics`: Expects an instance of `DatasetFeatureStatisticsList ` to compare with `lhs_statistics`.\n",
    "\n",
    "\n",
    "- `lhs_name`: Name of the `lhs_statistics` dataset.\n",
    "\n",
    "\n",
    "- `rhs_name`: Name of the `rhs_statistics` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f04fcd-758b-4693-837f-c38fce876741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation dataset statistics\n",
    "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
    "\n",
    "# Compare training evaluation\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=eval_stats,\n",
    "    rhs_statistics=train_stats,\n",
    "    lhs_name='EVAL_DATASET',\n",
    "    rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19243401-bb4d-4612-9a0f-8a00904bb024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculate and display evaluation anomalies\n",
    "\n",
    "You can use your reference schema to check for anomalies such as new values for a specific feature in the evaluation data. Detected anomalies can either be considered a real error that needs to be cleaned, or depending on your domain knowledge and the specific case, they can be accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62171780-e2ba-4710-9cba-2f38873c0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check evaluation data for errors by validating the evaluation dataset statistics using the reference schema\n",
    "anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
    "\n",
    "# Visualise anomalies\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac01df-73b5-4ea6-9dea-20433e34efac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering Pipelines\n",
    "\n",
    "You will build end-to-end pipelines in future courses but for this one, you will only build up to the feature engineering part. Specifically, you will:\n",
    "\n",
    "ingest data from a base directory with ExampleGen\n",
    "compute the statistics of the training data with StatisticsGen\n",
    "infer a schema with SchemaGen\n",
    "detect anomalies in the evaluation data with ExampleValidator\n",
    "preprocess the data into features suitable for model training with Transform\n",
    "If several steps mentioned above sound familiar, it's because the TFX components that deal with data validation and analysis (i.e. StatisticsGen, SchemaGen, ExampleValidator) uses Tensorflow Data Validation (TFDV) under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3781a1e-0867-4c40-b663-4d8243c14ccb",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82159d-85fa-4982-830c-e72b4ea5eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Pipeline\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20716a5-5f52-4e32-8304-f5c8873d254f",
   "metadata": {},
   "source": [
    "### Define paths\n",
    "\n",
    "You will define a few global variables to indicate paths in the local workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "# directory of the raw data files\n",
    "_data_root = './data/'\n",
    "\n",
    "# path to the raw training data\n",
    "_data_filepath = os.path.join(_data_root, 'A_E_Fire_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c9fa0",
   "metadata": {},
   "source": [
    "### Create the Interactive Context\n",
    "\n",
    "You will initialize the `InteractiveContext` below. This will create a database in the `_pipeline_root` directory which the different components will use to save or get the state of the component executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02228476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the InteractiveContext with a local sqlite file.\n",
    "# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n",
    "# You can safely ignore the warning about the missing config file.\n",
    "context = InteractiveContext(pipeline_root=_pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1eb8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run TFX components interactively\n",
    "\n",
    "With that, you can now run the pipeline interactively. You will see how to do that as you go through the different components below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc243fa",
   "metadata": {},
   "source": [
    "### ExampleGen\n",
    "\n",
    "You will start the pipeline with the [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component. This  will:\n",
    "\n",
    "*   split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
    "*   convert each data row into `tf.train.Example` format. This [protocol buffer](https://developers.google.com/protocol-buffers) is designed for Tensorflow operations and is used by the TFX components.\n",
    "*   compress and save the data collection under the `_pipeline_root` directory for other components to access. These examples are stored in `TFRecord` format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n",
    "\n",
    "Its constructor takes the path to your data source/directory. In our case, this is the `_data_root` path. The component supports several data sources such as CSV, tf.Record, and BigQuery. Since our data is a CSV file, we will use [CsvExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/CsvExampleGen) to ingest the data.\n",
    "\n",
    "Run the cell below to instantiate `CsvExampleGen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ExampleGen with the input CSV dataset\n",
    "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66af28-e30e-4e8f-89af-d6a00bb81539",
   "metadata": {},
   "source": [
    "You can execute the component by calling the `run()` method of the `InteractiveContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee13d2-830b-4fc2-b0f2-2079fec9f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the component\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c49adb-1478-44cc-ac3a-4afeb0820af7",
   "metadata": {},
   "source": [
    "You will notice that an output cell showing the execution results is automatically shown. This metadata is recorded into the database created earlier. This allows you to keep track of your project runs. For example, if you run it again, you will notice the `.execution_id` incrementing.\n",
    "\n",
    "The output of the components are called *artifacts* and you can see an example by navigating through  `.component.outputs > ['examples'] > Channel > ._artifacts > [0]` above. It shows information such as where the converted data is stored (`.uri`) and the splits generated (`.split_names`).\n",
    "\n",
    "You can also examine the output artifacts programmatically with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d086a-dfb6-499d-b40a-78982b6b2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the artifact object\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "\n",
    "# print split names and uri\n",
    "print(f'split names: {artifact.split_names}')\n",
    "print(f'artifact uri: {artifact.uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92222391-ea9f-42ea-aba5-33efa5bc9579",
   "metadata": {},
   "source": [
    "As mentioned, the ingested data is stored in the directory shown in the `uri` field. It is also compressed using `gzip` and you can verify by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a529d-f4b8-4468-b583-7f5ad835c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples\n",
    "train_uri = os.path.join(artifact.uri, 'Split-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c9e5a-a6bc-48c1-92b8-85d5b556691e",
   "metadata": {},
   "source": [
    "In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord), you will need to use methods that work with that data type. You will need to unpack the individual examples from the `TFRecord` file and format it for printing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699a1dd-bebd-47b6-afe6-73499b322c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                     for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a 'TFRecordDataset' to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3961c4-89da-4c87-ba2f-a8f649db49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get individual examples\n",
    "def get_records(dataset, num_records):\n",
    "    '''Extracts records from the given dataset.\n",
    "    Args:\n",
    "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
    "        num_records (int): number of records to preview\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty list\n",
    "    records = []\n",
    "    \n",
    "    # Use the 'take()' method to specify how many record to get\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        \n",
    "        # Get the numpy property of the tensor\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        \n",
    "        # Initialize a `tf.train.Example()` to read the serialized data\n",
    "        example = tf.train.Example()\n",
    "        \n",
    "        # Read the example data (output is a protocol buffer message)\n",
    "        example.ParseFromString(serialized_example)\n",
    "        \n",
    "        # covert the protocol buffer message to a Python dictionary\n",
    "        example_dict = (MessageToDict(example))\n",
    "        \n",
    "        # append to the records list\n",
    "        records.append(example_dict)\n",
    "        \n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a5379-ab44-4166-8411-8753977c4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 records from the dataset\n",
    "sample_records = get_records(dataset, 3)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51f6e6-8962-48e0-95c0-afeadfc68bf9",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab410-5b8c-49cf-b386-355e9ece8745",
   "metadata": {},
   "source": [
    "### StatisticsGen\n",
    "The [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component computes statistics over your dataset for data analysis, as well as for use in downstream components (i.e. next steps in the pipeline). As mentioned earlier, this component uses TFDV under the hood so its output will be familiar to you.\n",
    "\n",
    "`StatisticsGen` takes as input the dataset we just ingested using `CsvExampleGen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd7d69-520d-4c80-bf80-7ca36db68469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "\n",
    "# Execute the components\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfbca9-df1e-48da-8ebd-1a605f8cea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the output statistics\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfa307-8d92-4065-9da1-b24ac43e3c23",
   "metadata": {},
   "source": [
    "### SchemaGen\n",
    "\n",
    "The [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component also uses TFDV to generate a schema based on your data statistics.A schema defines the expected bounds, types, and properties of the features in your dataset.\n",
    "\n",
    "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055057a-f4f1-4ce4-855f-e282f1dcab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
    "schema_gen = tfx.components.SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],)\n",
    "\n",
    "# Run the component\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4693d7-d738-481c-a693-db2b23cfc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the schema\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6b8de-932a-4d37-9ceb-9ba0b34d3f71",
   "metadata": {},
   "source": [
    "### ExampleValidator\n",
    "\n",
    "The [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component detects anomalies in your data based on the generated schema from the previous step. Like the previous two components, it also uses TFDV under the hood. \n",
    "\n",
    "`ExampleValidator` will take as input the statistics from `StatisticsGen` and the schema from `SchemaGen`. By default, it compares the statistics from the evaluation split to the schema from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e167c53-2acb-4d6a-8e27-bffc1e904425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema = schema_gen.outputs['schema'])\n",
    "\n",
    "# Run the component.\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf75345-c736-43f9-827f-39180ccf0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31383f49-d86e-4ff4-bf24-720bdbc12adb",
   "metadata": {},
   "source": [
    "With no anomalies detected, you can proceed to the next step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c1d14-38de-4e30-ae71-8a20a753348e",
   "metadata": {},
   "source": [
    "### Transform\n",
    "The [Transform](https://www.tensorflow.org/tfx/guide/transform) component performs feature engineering for both training and serving datasets. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library introduced in the first ungraded lab of this week.\n",
    "\n",
    "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module containing the preprocessing function.\n",
    "\n",
    "In this section, you will work on an example of a user-defined Transform code. The pipeline needs to load this as a module so you need to use the magic command `%% writefile` to save the file to disk. Let's first define a few constants that group the data's attributes according to the transforms we will perform later. This file will also be saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0caf9c-8ae6-482b-8392-5079d7d1bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants module filename\n",
    "_aefire_constants_module_file = 'aefire_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb123d3-02f0-458f-b690-8bad7b499a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_aefire_constants_module_file}\n",
    "\n",
    "# Features with string data types that will be converted to indices\n",
    "CATEGORICAL_FEATURE_KEYS = ['FUEL']\n",
    "\n",
    "# Numerical features that are marked as continuous\n",
    "NUMERIC_FEATURE_KEYS = ['DESIBEL','FREQUENCY','SIZE']\n",
    "\n",
    "# Feature that can be grouped into buckets\n",
    "BUCKET_FEATURE_KEYS = ['AIRFLOW', 'DISTANCE']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each bucket feature.\n",
    "FEATURE_BUCKET_COUNT = {'AIRFLOW': 5, 'DISTANCE': 5}\n",
    "\n",
    "# Feature that the model will predict\n",
    "LABEL_KEY = 'STATUS'\n",
    "\n",
    "# Utility function for renaming the feature\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c0d53-d228-40b0-b6f8-ae837863defd",
   "metadata": {},
   "source": [
    "Next, you will work on the module that contains `preprocessing_fn()`. This function defines how you will transform the raw data into features that your model can train on (i.e. the next step in the pipeline). You will use the [tft module functions](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) to make these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee0a7b-cce3-4824-b035-6d058404f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transform module filename\n",
    "_aefire_transform_module_file = 'aefire_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c49bf-c9f5-49cb-a964-96284259b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_aefire_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import aefire_constants\n",
    "\n",
    "# Unpack the contents of the constants module\n",
    "_CATEGORICAL_FEATURE_KEYS = aefire_constants.CATEGORICAL_FEATURE_KEYS\n",
    "_NUMERIC_FEATURE_KEYS = aefire_constants.NUMERIC_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = aefire_constants.BUCKET_FEATURE_KEYS\n",
    "_FEATURE_BUCKET_COUNT = aefire_constants.FEATURE_BUCKET_COUNT\n",
    "_LABEL_KEY = aefire_constants.LABEL_KEY\n",
    "_transformed_name = aefire_constants.transformed_name\n",
    "\n",
    "# Define the transformantions\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "        Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    \n",
    "    # Convert strings to indices in a vocabulary\n",
    "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n",
    "    \n",
    "    # Scale these these feature to range [0,1]\n",
    "    for key in _NUMERIC_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_0_1(inputs[key])\n",
    "        \n",
    "    # Bucketize these features\n",
    "    for key in _BUCKET_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.bucketize(inputs[key], _FEATURE_BUCKET_COUNT[key])\n",
    "        \n",
    "    # Since the label has integer values, no need to convert\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ffec1-3517-4551-a658-dc746c2e3980",
   "metadata": {},
   "source": [
    "You can now pass the training data, schema, and transform module to the `Transform` component. You can ignore the warning messages generated by Apache Beam regarding type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55ad87-07b0-4b88-9cc4-14d9e8eeb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore TF warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Instantiate the Transform component\n",
    "transform = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_aefire_transform_module_file))\n",
    "    \n",
    "# Run the component\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4794a0d-809d-4814-bd50-0ba794d87f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cf2f662c261ce6e064fab84148fb7e431e494da40dd6352a5c1218f86b140eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
